{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SotaYoshida/Lecture_DataScience/blob/main/notebooks/Python_chapter_ArtificialNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKor3X3ievgT"
      },
      "source": [
        "# 機械学習における最適化\n",
        "\n",
        "この章では、最適化手法、中でもニューラルネットワークの訓練のための連続最適化手法を中心に解説する。\n",
        "\n",
        "理がない限りニューラルネットワークの構造としてはシンプルな多層パーセプトロン(MLP)を対象とし、損失関数としては回帰問題に対する平均二乗誤差、分類問題に対する交差エントロピー誤差を想定しているが、なるべく一般的な内容を扱うようにする。\n",
        "\n",
        "また、現代においても新たな最適化手法が提案され続けているため、ここでは代表的な手法を紹介するにとどめる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOxttwbLdc1E"
      },
      "source": [
        "## 勾配降下法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyeC4YIfjnua"
      },
      "source": [
        "\n",
        "勾配降下法(Gradient Descent, GD)は、関数の最小値を見つけるための最も基本的な最適化手法である。\n",
        "\n",
        "関数$f(\\boldsymbol{\\theta})$の勾配$\\nabla f(\\boldsymbol{\\theta})$は、関数が最も急激に増加する方向を示すベクトルであり、\n",
        "勾配降下法ではこの勾配の反対方向にパラメータ$\\boldsymbol{\\theta}$を更新することで、関数の値を減少させる、というのが基本的な考え方である。\n",
        "\n",
        "最も単純なものが最急降下法であり、パラメータ$\\boldsymbol{\\theta}$の更新は以下のように行われる。\n",
        "\n",
        "$$\n",
        "W_i := W_i - \\eta \\frac{\\partial L}{\\partial W_i}\n",
        "$$\n",
        "\n",
        "$L$は目的関数で、$\\eta$は学習率(パラメータ更新のスケールを決めるパラメータ)とした。\n",
        "\n",
        "ANN(MLP)の最適化においてはしばし、訓練データを小さなサブ集合(ミニバッチ)に分割し、各ミニバッチに対して勾配降下法を適用することが行われる。\n",
        "加えて、ミニバッチの中からランダムにサンプルを選ぶことも多く、これを確率的勾配降下法(Stochastic Gradient Descent, SGD)と呼ぶ。\n",
        "これにより、計算コストの削減と汎化性能の向上が期待できる。\n",
        "\n",
        "SGDなどの勾配法はシンプルで実装も容易であるが、中々収束しない、局所最適解に陥りやすい、などの問題が生じることもある。\n",
        "そうした経緯から提案されたのが、勾配だけでなく、過去の更新情報を利用するモーメンタム法や、パラメータごとに異なる学習率を適応的に調整するAdaGrad, RMSProp, Adamなどの手法である。\n",
        "年代に沿ってこれらの手法を紹介するのも楽しいが、続く節では、現代においても広く用いられているAdamを中心に説明する。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3NSz6JulqFB"
      },
      "source": [
        "## Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0QGVWTmltgl"
      },
      "source": [
        "Adamは、勾配降下法の様にその都度の勾配の情報だけを使うのではなく、以前の勾配の情報も有効活用しながら学習率も調整する手法である。\n",
        "\n",
        "2014年にDiederik P. KingmaとJimmy Baによって提案されて以来、深層学習の分野で現代に至るまで広く用いられている。→[arXiv:1412.6980](https://arxiv.org/abs/1412.6980)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AdamW\n",
        "\n",
        "AdamWは、Adamの変種であり、L2正則化を直接パラメータ更新に組み込むことで、過学習を防止する手法である。\n",
        "名前のWはWeight decay(重み減衰)に由来する。このweight decayは過学習を防止するための正則化手法の一つであり、パラメータの大きさを抑制するために用いられる。\n",
        "損失関数にパラメータの二乗和を加えることで実現される。\n",
        "\n",
        "これは機械学習分野ではL2正則化と呼ばれるが、SGDの場合と異なり、Adamに対して単純にパラメータ更新の式にL2正則化項を加えてしまうと、重み減衰の効果が正しく働かないことが指摘された。\n",
        "そこで提案されたのがAdamWである。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 二階微分を用いる方法\n",
        "\n",
        "勾配のみを用いる手法は、関数の形状に関する情報が限られているため、収束が遅い、局所最適解に陥りやすい、などの問題がある。これに対処するために、ヘッセ行列を用いた二階微分の情報を活用する方法がある。\n",
        "二階微分の情報を用いることで、関数の曲率に関する情報を得ることができ、より効率的な最適化が可能になる。\n",
        "一方で、ヘッセ行列の計算は計算コストが高く、特に高次元のパラメータ空間では実用的でないことが多い。\n",
        "\n",
        "代表的な手法として、ニュートン法や準ニュートン法(L-BFGS法, etc.)などがある。\n",
        "ニュートン法では、ヘッセ行列の逆行列を用いてパラメータを更新するのに対して、\n",
        "準ニュートン法では、ヘッセ行列の近似を用いることで計算コストを削減する違いがある。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 残差接続\n",
        "\n",
        "最適化手法とは異なるが、ニューラルネットワークの構造として残差接続(Residual Connection)が提案されている。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## $\\clubsuit$ EMアルゴリズム\n",
        "\n",
        "Expectation maximization (EM)アルゴリズムは、観測データに対して潜在変数が存在する場合に、パラメータの最尤推定を行うための反復的な手法である。\n",
        "とくに、混合ガウスモデルや隠れマルコフモデルなどの確率モデルで広く用いられている他、\n",
        "最近では、変分オートエンコーダ(Variational Autoencoder, VAE)などの深層生成モデルにも応用されている。\n",
        "\n",
        "以下では、複数のガウス分布の線形結合でデータをモデル化する**ガウス混合モデル**を例に、EMアルゴリズムの基本的な考え方を説明する。\n",
        "\n",
        "初めに、各ガウス分布のパラメータを $\\boldsymbol{\\mu}_k$ (平均)、$\\boldsymbol{\\Sigma}_k$ (共分散行列)、および混合係数 $\\pi_k$ (各ガウス分布の寄与度) とする。\n",
        "\n",
        "* **Eステップ**: 現在のパラメータを用いて、各データ点$n$がどのガウス分布$k$に属するかの確率を計算する。\n",
        "    \n",
        "    $$\n",
        "    r_{nk} = \\frac{\\pi_k \\mathcal{N}(\\boldsymbol{x}_n | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\n",
        "        \\sum_j \\pi_j  \\mathcal{N}(\\boldsymbol{x}_n | \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)\n",
        "    }\n",
        "    $$\n",
        "\n",
        "* **Mステップ**: Eステップで得られた確率$r$を用いて、ガウス分布のパラメータ $\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k, \\pi_k$ を更新する。\n",
        "\n",
        "    $$\n",
        "    \\boldsymbol{\\mu}_k = \\frac{1}{N_k} \\sum_n r_{nk} \\boldsymbol{x}_n, \\quad\n",
        "    \\boldsymbol{\\Sigma}_k = \\frac{1}{N_k} \\sum_n r_{nk} (\\boldsymbol{x}_n - \\boldsymbol{\\mu}_k)(\\boldsymbol{x}_n - \\boldsymbol{\\mu}_k)^\\top, \\quad\n",
        "    \\pi_k = \\frac{N_k}{N}\n",
        "    $$\n",
        "\n",
        "この逐次更新によって対数尤度の最大化を図り、収束するまでEステップとMステップを繰り返す。\n",
        "対数尤度は以下のように表される。\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\boldsymbol{\\theta}) = \\sum_n \\log \\left( \\sum_k \\pi_k \\mathcal{N}(\\boldsymbol{x}_n | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\right)\n",
        "$$"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "authorship_tag": "ABX9TyPpQYMbqGEZMRisgxppnqSm",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Python_chapter_ArtificialNeuralNetwork.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
